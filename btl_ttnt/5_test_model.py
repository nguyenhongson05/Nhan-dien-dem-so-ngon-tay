import cv2
import mediapipe as mp
import os
from ultralytics import YOLO

# Load m√¥ h√¨nh YOLO ƒë√£ hu·∫•n luy·ªán
print("üöÄ ƒêang t·∫£i m√¥ h√¨nh YOLO...")
model = YOLO("runs/detect/train4/weights/best.pt")
print("‚úÖ M√¥ h√¨nh YOLO ƒë√£ t·∫£i xong!")

# Kh·ªüi t·∫°o MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=1)

# Danh s√°ch ID c·ªßa c√°c ƒë·∫ßu ng√≥n tay trong MediaPipe
TIP_IDS = [4, 8, 12, 16, 20]

def identify_hand_gesture(hand_landmarks):
    fingers = [0, 0, 0, 0, 0]
    
    if hand_landmarks.landmark[TIP_IDS[0]].x > hand_landmarks.landmark[TIP_IDS[0] - 1].x:
        fingers[0] = 1
    
    for i in range(1, 5):
        if hand_landmarks.landmark[TIP_IDS[i]].y < hand_landmarks.landmark[TIP_IDS[i] - 2].y:
            fingers[i] = 1
    
    if fingers == [1, 1, 0, 0, 1]:  # Ng√≥n √°p √∫t v√† gi·ªØa g·∫≠p xu·ªëng
        return 6
    elif fingers == [1, 0, 0, 0, 1]:  # Ch·ªâ gi∆° ng√≥n c√°i v√† √∫t
        return 7
    elif fingers == [1, 1, 0, 1, 1]:  # G·∫≠p ng√≥n gi·ªØa
        return 8
    elif fingers == [0, 1, 1, 0, 1] and hand_landmarks.landmark[TIP_IDS[0]].y > hand_landmarks.landmark[TIP_IDS[3]].y:
        return 9  # Ng√≥n c√°i ch·∫°m ng√≥n √°p √∫t
    elif fingers == [0, 1, 0, 1, 1] and hand_landmarks.landmark[TIP_IDS[0]].y > hand_landmarks.landmark[TIP_IDS[2]].y:
        return 10  # Ng√≥n c√°i ch·∫°m ng√≥n gi·ªØa
    
    return sum(fingers)

def get_finger_bounding_box(hand_landmarks, frame):
    min_x = min_y = float('inf')
    max_x = max_y = float('-inf')
    
    for i in TIP_IDS:
        x, y = int(hand_landmarks.landmark[i].x * frame.shape[1]), int(hand_landmarks.landmark[i].y * frame.shape[0])
        min_x, min_y = min(min_x, x), min(min_y, y)
        max_x, max_y = max(max_x, x), max(max_y, y)
    
    return min_x - 20, min_y - 20, max_x + 20, max_y + 20

# T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh n·∫øu ch∆∞a c√≥
save_dir = "captured_images/"
os.makedirs(save_dir, exist_ok=True)

count = 0  # Bi·∫øn ƒë·∫øm ·∫£nh

cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("‚ùå Kh√¥ng th·ªÉ m·ªü camera!")
    exit()
else:
    print("‚úÖ Camera ƒë√£ m·ªü th√†nh c√¥ng!")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera!")
        break
    
    frame = cv2.flip(frame, 1)
    results = model(frame)
    hand_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    hand_results = hands.process(hand_rgb)
    
    if hand_results.multi_hand_landmarks:
        for hand_landmarks in hand_results.multi_hand_landmarks:
            gesture = identify_hand_gesture(hand_landmarks)
            cv2.putText(frame, f'Finger number: {gesture}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            x1, y1, x2, y2 = get_finger_bounding_box(hand_landmarks, frame)
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    else:
        cv2.putText(frame, 'No finger', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    
    cv2.imshow("Nh·∫≠n di·ªán tay v√† c·ª≠ ch·ªâ", frame)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('s'):  # L∆∞u ·∫£nh khi ·∫•n 's'
        filename = os.path.join(save_dir, f"captured_{count}.jpg")
        cv2.imwrite(filename, frame)
        print(f"‚úÖ ·∫¢nh ƒë√£ l∆∞u: {filename}")
        count += 1

cap.release()
cv2.destroyAllWindows()
